from langchain.tools import tool
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

from ..core.config import EPM_URL, OPENAI_API_KEY, FAISS_INDEX_PATH
from ..scraping.epm_scraper import fetch_interrupciones_html
from ..scraping.epm_parser import parse_interrupciones

@tool
def obtener_info_cortes_de_agua() -> str:
    """
    칔til para responder preguntas sobre interrupciones o cortes del servicio de agua.
    Consulta la p치gina oficial de EPM para obtener informaci칩n actualizada sobre las interrupciones programadas.
    """
    print("游댢 Usando la herramienta de cortes de agua...")
    html = fetch_interrupciones_html(EPM_URL)
    info = parse_interrupciones(html)
    return info

@tool
def buscar_en_guias_epm(pregunta: str) -> str:
    """
    칔til para responder preguntas generales sobre EPM que NO est치n relacionadas con cortes de agua.
    Busca en documentos y gu칤as internas sobre temas como facturaci칩n, tr치mites, reportar da침os, etc.
    """
    print("游닄 Usando la herramienta de b칰squeda en gu칤as...")
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

    # Carga el 칤ndice local de FAISS (nuestra 'memoria')
    vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)

    # Busca los documentos m치s relevantes para la pregunta
    retriever = vector_store.as_retriever(search_kwargs={"k": 3}) # k=3 trae los 3 chunks m치s relevantes
    docs_relevantes = retriever.invoke(pregunta)

    # Une el contenido de los documentos relevantes en un solo string
    contexto = "\n---\n".join([doc.page_content for doc in docs_relevantes])
    return contexto